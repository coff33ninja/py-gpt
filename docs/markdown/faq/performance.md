# Performance FAQ

Common questions about PyGPT performance and optimization.

## ğŸš€ Speed & Performance

### Q: Why are responses slow?

**A:** Several factors affect response speed:

**Model Speed:**
- GPT-4: Slower but higher quality
- GPT-3.5/Gemini Flash: Much faster
- Local models: Depends on hardware

**Solutions:**
```
âœ… Use faster models (GPT-3.5, Gemini Flash)
âœ… Reduce max tokens
âœ… Shorter prompts
âœ… Disable unnecessary plugins
âœ… Check internet connection
```

### Q: How can I make PyGPT faster?

**A:** Optimization strategies:

**1. Model Selection:**
```
Fast models:
- Gemini Flash (fastest)
- GPT-3.5-turbo
- Mistral models
- DeepSeek
```

**2. Settings:**
```
- Reduce max tokens: 512-1024
- Lower temperature: 0.3-0.5
- Disable streaming if not needed
- Enable caching
```

**3. Plugins:**
```
- Disable unused plugins
- Limit tool calls
- Optimize plugin settings
```

**4. System:**
```
- Close other applications
- Good internet connection
- Sufficient RAM
- SSD storage
```

### Q: Why is the interface laggy?

**A:** UI performance issues:

**Causes:**
- Large conversation history
- Many contexts loaded
- Heavy plugins active
- Insufficient system resources

**Solutions:**
```
âœ… Clear old contexts
âœ… Reduce loaded contexts
âœ… Disable heavy plugins
âœ… Use compact mode
âœ… Reduce font size
âœ… Disable animations
```

### Q: Code execution is slow

**A:** Code Interpreter optimization:

**For Local Execution:**
```
- Use faster Python interpreter
- Optimize code
- Reduce library imports
- Cache results
```

**For Docker:**
```
- Use local execution instead
- Allocate more resources
- Use faster base image
- Keep container running
```

## ğŸ’¾ Memory & Resources

### Q: PyGPT uses too much RAM

**A:** Memory optimization:

**Causes:**
- Many contexts in memory
- Large conversation history
- Heavy plugins
- Memory leaks

**Solutions:**
```
âœ… Close unused contexts
âœ… Clear old conversations
âœ… Disable heavy plugins
âœ… Restart PyGPT regularly
âœ… Use 64-bit version
âœ… Increase system RAM
```

**Memory Usage:**
```
Minimal: ~200MB
Normal: ~500MB
Heavy use: ~1-2GB
With local models: 4-16GB+
```

### Q: High CPU usage

**A:** CPU optimization:

**Causes:**
- Streaming responses
- Syntax highlighting
- Multiple plugins
- Background tasks

**Solutions:**
```
âœ… Disable streaming
âœ… Reduce syntax highlighting
âœ… Disable unused plugins
âœ… Close other applications
âœ… Update to latest version
```

### Q: Disk space issues

**A:** Storage management:

**What Uses Space:**
```
- Context database: 10MB-1GB+
- Vector indexes: 100MB-10GB+
- Cache files: 10-100MB
- Logs: 1-10MB
- Backups: Varies
```

**Solutions:**
```
âœ… Delete old contexts
âœ… Clear vector indexes
âœ… Clear cache
âœ… Rotate logs
âœ… Remove old backups
âœ… Use external storage
```

## ğŸŒ Network & API

### Q: API calls are slow

**A:** Network optimization:

**Check:**
```
1. Internet speed
2. API endpoint location
3. Provider status
4. Rate limits
5. Firewall/proxy
```

**Solutions:**
```
âœ… Use closer endpoint
âœ… Check provider status
âœ… Upgrade internet
âœ… Use VPN if blocked
âœ… Batch requests
```

### Q: Frequent timeouts

**A:** Timeout handling:

**Causes:**
- Slow internet
- Large requests
- Provider issues
- Firewall blocking

**Solutions:**
```
âœ… Increase timeout: 60-120s
âœ… Reduce request size
âœ… Check provider status
âœ… Use different provider
âœ… Enable retries
```

### Q: Rate limit errors

**A:** Rate limit management:

**Understanding Limits:**
```
OpenAI: 60 req/min (varies by tier)
Google: Varies by model
Anthropic: Varies by plan
```

**Solutions:**
```
âœ… Implement delays
âœ… Batch requests
âœ… Upgrade API plan
âœ… Use multiple keys
âœ… Monitor usage
```

## ğŸ” Search & Indexing

### Q: Vector search is slow

**A:** Index optimization:

**Causes:**
- Large index
- Slow vector store
- Many documents
- Poor configuration

**Solutions:**
```
âœ… Use faster vector store (Qdrant, Pinecone)
âœ… Reduce chunk size
âœ… Limit top K results
âœ… Enable caching
âœ… Use local embeddings
```

### Q: Indexing takes forever

**A:** Indexing optimization:

**Speed Up:**
```
âœ… Batch documents
âœ… Use faster embedding model
âœ… Reduce chunk overlap
âœ… Parallel processing
âœ… Use local embeddings
```

**Time Estimates:**
```
100 pages: 1-5 minutes
1000 pages: 10-30 minutes
10000 pages: 1-3 hours
```

### Q: Web search is slow

**A:** Search optimization:

**Causes:**
- Many results requested
- Slow provider
- Content extraction
- Network latency

**Solutions:**
```
âœ… Reduce max results: 5-10
âœ… Use faster provider (DuckDuckGo)
âœ… Disable content extraction
âœ… Use cached results
```

## ğŸ¤– Agent Performance

### Q: Agents are too slow

**A:** Agent optimization:

**Causes:**
- Many steps
- Complex tasks
- Tool calls
- Evaluation enabled

**Solutions:**
```
âœ… Reduce max steps: 20-30
âœ… Simplify tasks
âœ… Disable evaluation
âœ… Use faster model
âœ… Limit tool calls
```

### Q: Agent gets stuck

**A:** Agent troubleshooting:

**Causes:**
- Infinite loops
- Unclear goals
- Tool failures
- Timeout issues

**Solutions:**
```
âœ… Set lower max steps
âœ… Clearer instructions
âœ… Enable timeout
âœ… Monitor progress
âœ… Stop and restart
```

## ğŸ’» Local Models

### Q: Ollama is very slow

**A:** Local model optimization:

**Hardware Requirements:**
```
Minimum:
- 8GB RAM
- 4 CPU cores
- SSD storage

Recommended:
- 16GB+ RAM
- 8+ CPU cores
- GPU (NVIDIA)
- NVMe SSD
```

**Optimization:**
```
âœ… Use GPU acceleration
âœ… Smaller models (7B vs 70B)
âœ… Reduce context length
âœ… Quantized models
âœ… Optimize system
```

**Model Sizes:**
```
7B: Fast, good quality
13B: Balanced
30B: Slower, better quality
70B: Very slow, best quality
```

### Q: GPU not being used

**A:** GPU acceleration:

**Check:**
```
1. GPU drivers installed
2. CUDA/ROCm installed
3. Ollama configured for GPU
4. Model supports GPU
```

**Enable GPU:**
```
Ollama: Automatic if available
Check: nvidia-smi (NVIDIA)
Check: rocm-smi (AMD)
```

## ğŸ“Š Monitoring

### Q: How to monitor performance?

**A:** Performance monitoring:

**Built-in Tools:**
```
- Token counter
- Response time
- API usage stats
- Debug logs
```

**System Tools:**
```
- Task Manager (Windows)
- Activity Monitor (Mac)
- htop (Linux)
- Network monitor
```

**Metrics to Watch:**
```
- Response time
- Token usage
- Memory usage
- CPU usage
- Network speed
- API costs
```

## ğŸ”§ Optimization Checklist

### Quick Wins

```
â˜‘ Use faster models
â˜‘ Reduce max tokens
â˜‘ Disable unused plugins
â˜‘ Clear old contexts
â˜‘ Enable caching
â˜‘ Good internet connection
â˜‘ Close other apps
â˜‘ Update PyGPT
```

### Advanced Optimization

```
â˜‘ Optimize prompts
â˜‘ Batch operations
â˜‘ Use local models
â˜‘ Configure vector store
â˜‘ Tune model parameters
â˜‘ Monitor usage
â˜‘ Profile performance
â˜‘ Hardware upgrade
```

## ğŸ”— Related Resources

- [Advanced Settings](../guides/07-advanced-settings.md)
- [Troubleshooting](../reference/troubleshooting.md)
- [Configuration Reference](../reference/config-reference.md)

## ğŸ†˜ Still Having Issues?

- Check [Troubleshooting Guide](../reference/troubleshooting.md)
- Visit [General FAQ](./general.md)
- Ask on [Discord](https://pygpt.net/discord)
- Report issues on [GitHub](https://github.com/szczyglis-dev/py-gpt/issues)

---

**Last Updated:** February 2026
